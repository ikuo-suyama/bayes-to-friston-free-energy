
## はじめに

Karl Fristonの自由エネルギー原理（Free Energy Principle）について勉強していて、「なぜ『自由エネルギー』なのか」「ベイズ推論とどう繋がるのか」という疑問を持ちました。本記事は、その疑問を解決するために**ChatGPT 5.2と対話しながら学んだ内容を整理した学習ログ**です。

**ベイズの定理という基本原理から出発し、変分自由エネルギーが必然的に導出されるプロセス**を、数式を追いながら理解することを目指しました。さらに、統計力学の自由エネルギーとの数学的同型性を確認し、Fristonの理論の位置づけを明らかにします。

**本記事で扱う内容：**
- ベイズ推論における「驚き（surprisal）」の意味と計算不可能性
- 変分推論における自由エネルギーの2つの導出方法（Jensen不等式とKL分解）
- 統計力学とベイズ推論の数学的対応関係
- Fristonの自由エネルギー原理の位置づけと意義

**重要な注意事項：**
- 本記事はChatGPT 5.2の助けを借りて書いており、**誤りが含まれる可能性があります**
- 数学的な導出については可能な限り確認していますが、解釈や理解に誤りがあるかもしれません
- **誤りや改善点のご指摘を歓迎します**。コメントやフィードバックをいただけると幸いです


## 要約：何が分かったのか

ベイズの定理から出発すると、

**変分自由エネルギーは「計算できない驚き $${{-\log p(x)}}$$ を、計算可能な上界として評価する量」として必然的に導出される。**

**その数式構造は、統計力学における自由エネルギーの変分原理と完全に同型であり、Fristonの自由エネルギー原理は、この同型性を推論・認知・生命の原理として再解釈したものである。**


## 詳細な導出

## 1. 出発点：ベイズの定理は「世界の生成過程」

まずベイズの定理そのものから始める。

$$
p(\theta \mid x) = \frac{p(x,\theta)}{p(x)} = \frac{p(x\mid\theta)p(\theta)}{p(x)}
$$

ここで重要なのは：

- $${{\theta}}$$：世界の隠れた状態（原因・仮説）
- $${{x}}$$：観測データ（結果）

そしてベイズが仮定しているのは**独立ではなく生成順序**：

$$
\theta \sim p(\theta),\quad x \sim p(x\mid\theta)
$$

この2つを合わせた

$$
p(x,\theta)=p(x\mid\theta)p(\theta)
$$

が**生成モデル**。


## 2. 驚き：なぜ -log p(x) が問題になるのか

ここで改めて問い直したいのは、次の一点だ。

**観測者にとって「本当に問題になる量」は何か？**

ベイズ推論の文脈で、観測者が直接手にしているのはデータ $${{x}}$$ だけであり、世界の真の状態やパラメータ $${{\theta}}$$ は決して直接観測できない。

### 周辺尤度 p(x) とはなにか

まず、ベイズ推論が置いている世界観を確認しよう。

生成モデルでは、世界は次の順序で生成されると仮定されている：

$$
\theta \sim p(\theta), \qquad x \sim p(x \mid \theta)
$$

すなわち、
- $${{\theta}}$$：世界の隠れた状態（原因・仮説）
- $${{x}}$$：その結果として生成される観測データ

これを合わせた同時分布が

$$
p(x,\theta) = p(x\mid\theta)p(\theta)
$$

である。

しかし、観測者の立場に立つと重要な事実がある。

**どの $${{\theta}}$$ が真だったのかは分からない。分かるのは「$${{x}}$$ が起きた」という事実だけである。**

このとき、「この生成モデルのもとで、$${{x}}$$ が観測される確率」を定義する唯一の方法は、全確率の法則を使うことだ。

離散なら和、連続なら積分になる：

$$
p(x) = \int p(x\mid\theta)p(\theta)d\theta
$$

これは特別な操作ではなく、

> 「どの世界にいるか分からない以上、あり得るすべての世界（$${{\theta}}$$）の下での起こりやすさを信念 $${{p(\theta)}}$$ で重み付けして平均する」

という、ごく自然な帰結である。

言い換えると、

$$
p(x) = \mathbb{E}_{p(\theta)}[p(x\mid\theta)]
$$

であり、**周辺尤度とは「世界全体としての予測分布」** にほかならない。

### なぜそれが「驚き」になるのか

では、なぜこの $${{p(x)}}$$ に対して $${{-\log}}$$ を取った量が特別な意味を持つのか。

情報理論では、ある事象 $${{x}}$$ が起きたときの**自己情報量（surprisal）** は

$$
\text{Surprisal}(x) = -\log p(x)
$$

と定義される。

これは単なる命名ではない。
- 起こりにくい事象ほど「情報量」が大きい
- 独立な事象では情報量が加算される
- 確率に対して連続的に変化する

これらの自然な要請を満たす関数として、情報理論では $${{-\log}}$$ が標準的に用いられている（Shannon）。

### 尤度との決定的な違い

ここで重要な区別がある。

- **尤度** $${{p(x\mid\theta)}}$$：「この $${{\theta}}$$ が真だと仮定したとき、この $${{x}}$$ はどれくらい起こりにくいか」
- **驚き** $${{-\log p(x)}}$$：「どの $${{\theta}}$$ が真か分からない状況で、この生成モデル全体から見て、この $${{x}}$$ はどれくらい起こりにくいか」

つまり、

$$
-\log p(x) \neq -\log p(x\mid\hat\theta)
$$

驚きとは、特定の仮説に対する違和感ではなく、**自分が信じていた"世界全体"に対する想定外度**なのである。

### なぜ「問題」になるのか

ここでようやく核心に来る。

$$
-\log p(x) = -\log \int p(x,\theta)d\theta
$$

この量は、
- 観測者にとって最も意味のある評価量でありながら
- 高次元積分を含み
- 実用上ほぼ計算不能

という、極めて厄介な性質を持つ。

**驚きは定義できるが、直接は計算できない。**

この一点こそが、変分推論・自由エネルギー・ELBOが導入される必然的な理由である。

### 補足：周辺尤度が正規化定数になる必然性

ここまでで、周辺尤度 $${{p(x)}}$$ が積分として現れることを見てきた。しかし、この同じ量がベイズの定理で**正規化定数**としても働く。これは偶然だろうか？

答えは**必然**である。

**同時分布はすでに正規化されている**

生成モデル $${{p(x,\theta)}}$$ は確率分布なので、定義として

$$
\int\int p(x,\theta)dxd\theta = 1
$$

を満たしている。全体は最初から正規化されている。

**周辺化は「世界の一部を捨てる操作」**

$${{\theta}}$$ が観測できないので、全確率の法則で積分する：

$$
p(x)=\int p(x,\theta)d\theta
$$

これは新しい仮定を入れておらず、単に「見えない変数を足し合わせただけ」である。

**なぜそれが「正規化」になるのか**

事後分布 $${{p(\theta\mid x)}}$$ を定義したい。これは「$${{\theta}}$$ の確率分布」なので、必ず

$$
\int p(\theta\mid x)d\theta = 1
$$

でなければならない。

ベイズの定理を書くと：

$$
p(\theta\mid x)=\frac{p(x,\theta)}{p(x)}
$$

ここで分母に入る量は $${{\int p(x,\theta)d\theta}}$$ 以外にあり得ない。なぜなら：

$$
\int \frac{p(x,\theta)}{C(x)}d\theta = 1 \quad\Rightarrow\quad C(x)=\int p(x,\theta)d\theta
$$

正規化条件を満たすために一意に決まる。

**「偶然っぽく見える」理由**

私たちは普段、「まず尤度×事前を書く」→「あとで割って正規化する」という順で考えるため、「たまたま同じ積分が正規化定数にもなっている」ように見える。

しかし論理順は逆である。

「$${{\theta}}$$ について確率分布にしたい」→ 正規化が必要 → その結果、周辺尤度が出てくる

**向きを変えると見え方が変わる**

同じ量 $${{p(x) = \int p(x,\theta)d\theta}}$$ を、2つの立場で見ると：

- **$${{\theta}}$$ を変数に見ると**：正規化定数（事後分布を作るために必要）
- **$${{x}}$$ を変数に見ると**：予測分布（周辺尤度・エビデンス）

役割が違うだけで、量は同じである。

つまり、**「周辺化」と「正規化」は同じ操作を、見る向き（$${{x}}$$ 側か $${{\theta}}$$ 側か）を変えて呼んでいるだけ** である。


## 3. 近道を作る：変分分布 q(θ) の導入

前置きが長くなった。問題は $${{p(x)}}$$ が計算困難である点であった。そこで発想を変える。

- 真の事後分布 $${{p(\theta\mid x)}}$$ は分からない
- 代わりに、任意の分布 $${{q(\theta)}}$$ を置く

積分を期待値として書き換える：

$$
p(x) = \int q(\theta)\frac{p(x,\theta)}{q(\theta)}d\theta = \mathbb{E}_q\left[\frac{p(x,\theta)}{q(\theta)}\right]
$$


## 4. Jensenの不等式：上界が自然に出てくる

$${{-\log}}$$ は凸関数なのでJensenを適用できる：

$$
-\log p(x) = -\log \mathbb{E}_q\left[\frac{p(x,\theta)}{q(\theta)}\right] \le \mathbb{E}_q\left[-\log \frac{p(x,\theta)}{q(\theta)}\right]
$$

右辺を整理すると：

$$
\mathcal{F}(q) = \mathbb{E}_q[-\log p(x,\theta)] + \mathbb{E}_q[\log q(\theta)]
$$

これが**変分自由エネルギー**。

不等式の関係から、$${{\mathcal{F}(q) \ge -\log p(x)}}$$ となっており、変分自由エネルギーは驚き $${{-\log p(x)}}$$ の上界を与えている。この上界は機械学習では**ELBO（Evidence Lower BOund：エビデンス下界）** と呼ばれることが多い。符号を反転させると、$${{-\mathcal{F}(q) \le \log p(x)}}$$ となり、これが「エビデンス（対数周辺尤度）の下界」という名前の由来である。


## 5. KL分解：自由エネルギーの正体が見える

さらに変分自由エネルギーは、次のように分解できる：

$$
\mathcal{F}(q) = -\log p(x) + \mathrm{KL}\left( q(\theta)|p(\theta\mid x) \right)
$$

ここで初めて、自由エネルギーの中身がはっきり見えてくる。

- $${{-\log p(x)}}$$：この生成モデル全体から見た**真の驚き**
- $${{\mathrm{KL}}}$$：自分の近似 $${{q(\theta)}}$$ が、真の事後分布からどれだけズレているか

### KLダイバージェンスとは何か

KLダイバージェンス（Kullback–Leibler divergence）は、2つの確率分布 $${{q(\theta)}}$$ と $${{p(\theta)}}$$ の「近さ」を測る量で、定義は次の通り：

$$
\mathrm{KL}(q|p) = \int q(\theta) \log\frac{q(\theta)}{p(\theta)}d\theta
$$

KLは次の重要な性質を持つ：

- 常に $${{\mathrm{KL}(q|p)\ge 0}}$$
- 等号成立（KL=0）は $${{q(\theta)=p(\theta)}}$$ のとき

つまり、**KLは「完全一致からどれだけズレているか」を測る量**であり、0になったときだけ「同じ分布」と言える。

### なぜ「距離っぽい」と言われるのか（ただし距離ではない）

KLはしばしば「距離」と説明されるが、正確には距離ではない。

- **対称でない**：$${{\mathrm{KL}(q|p)\neq \mathrm{KL}(p|q)}}$$
- 三角不等式も成り立たない

それでも「近さ」を表すと感じられる理由は、

> 「$${{p}}$$ を真実だと思って世界を符号化したとき、$${{q}}$$ を使ってしまうことで、どれだけ余計な情報量（無駄な驚き）を支払うか」

を測っているからだ。

情報理論的には：

- $${{\mathbb{E}_q[-\log p]}}$$：$${{p}}$$ を使って符号化したときの平均コード長
- $${{\mathbb{E}_q[-\log q]}}$$：最適なコード長

その差がKL。**「間違った世界観を持つコスト」** と読める。

### 自由エネルギーのKL分解が意味すること

もう一度式を見る：

$$
\mathcal{F}(q) = -\log p(x) + \mathrm{KL}\left( q(\theta)|p(\theta\mid x) \right)
$$

この式が言っているのは、非常に明確だ。

- 観測 $${{x}}$$ が与えられたあと、$${{-\log p(x)}}$$ は**固定された定数**
- それにもかかわらず $${{\mathcal{F}(q)}}$$ が大きいのは、自分の信念 $${{q(\theta)}}$$ がまだズレているから

したがって：

$$
\mathcal{F}(q) \ge -\log p(x)
$$

これは偶然の不等式ではなく、

$$
\text{自由エネルギー} = \text{真の驚き} + \text{認識のズレ}
$$

という分解そのもの。

### 「自由エネルギーは驚きの上界」の正確な意味

ここで言う「上界」とは、

- 世界が本当にどれくらい驚くべきだったか（$${{-\log p(x)}}$$）
- 自分がそれをどれくらい正確に評価できているか（$${{\mathrm{KL}}}$$）

の合成量である、という意味だ。

自由エネルギーを最小化すると起きるのは：

- 驚きそのものが減る（わけではない）
- 驚きの見積もり誤差（$${{\mathrm{KL}}}$$）が減る

つまり、

> **「自分がどれだけ驚くべきだったか」を正しく理解する方向に信念が更新される**

これが、変分推論における「理解が進む」という状態である。

さらに、行動やモデル更新を含めると、将来の $${{p(x)}}$$ 自体を変えにいくことになる。これがFristonの自由エネルギー原理における「能動的推論（active inference）」の基礎である。


## 6. 統計力学との数式レベルの完全対応

### 数式の対応

統計力学（Gibbs）：

$$
F[q] = \mathbb{E}q[E(s)] - TS[q] = F{\text{eq}} + T\mathrm{KL}(q|p)
$$

ベイズ変分推論：

$$
\mathcal{F}[q] = \mathbb{E}_q[-\log p(x,\theta)] + \mathbb{E}_q[\log q] = -\log p(x) + \mathrm{KL}(q|p)
$$

- 統計力学の「状態 $${s}$$」 ↔ ベイズの「潜在変数 $${\theta}$$」
- 統計力学の「エネルギー $${E}$$」 ↔ ベイズの「$${-\log p(x,\theta)}$$」
- 統計力学の「分配関数 $${Z}$$」 ↔ ベイズの「周辺尤度 $${p(x)}$$」
- 統計力学の「自由エネルギー」 ↔ ベイズの「変分自由エネルギー」
- 統計力学の「平衡分布」 ↔ ベイズの「事後分布」

**数学的に完全同型**。

しかし、この対応が本当に面白くなるのは、**「なぜその形になるのか」** を意味レベルで比較したときだ。

### 統計力学：なぜ「低エネルギー」だけでは決まらないのか

統計力学で系の状態 $${{s}}$$ は、エネルギー $${{E(s)}}$$ によって特徴づけられる。

直感的には「エネルギーが低い状態ほど起こりやすい」と思いたくなるが、実際にはそれだけではない。

ボルツマン分布は：

$$
p(s) = \frac{1}{Z} e^{-\beta E(s)}, \quad \beta=\frac{1}{T}
$$

ここで重要なのは：

- 低エネルギー状態でも**状態の数が少なければあまり起こらない**
- 多少エネルギーが高くても**状態数（エントロピー）が多ければ頻繁に起こる**

つまり系は、

> **エネルギーを下げたい欲求**と**状態数を増やしたい欲求（エントロピー）** のトレードオフとして振る舞う

このトレードオフを一つの量にまとめたものが：

$$
F[q] = \mathbb{E}_q[E(s)] - T S[q]
$$

であり、自由エネルギーを最小にする分布が、実際に観測される分布になる。

### ベイズ変分推論：なぜ「尤度最大」だけでは足りないのか

同じ構造は、ベイズ推論にもそのまま現れる。

ベイズ推論では：

- $${{-\log p(x,\theta)}}$$：「この仮説の下で、データがどれくらい不自然か」
- $${{-\mathbb{E}_q[\log q]}}$$：「どれくらい多様な仮説を許しているか」

がせめぎ合う。

変分自由エネルギー：

$$
\mathcal{F}[q] = \mathbb{E}_q[-\log p(x,\theta)] + \mathbb{E}_q[\log q(\theta)]
$$

は、

> **データをよく説明したい（驚きを下げたい）** と**過度に一点に固執したくない（不確実性を残したい）**

というトレードオフを一つの量にまとめたものだ。

これはまさに統計力学と同じ構造で、

- エネルギー ↔ データ不適合度
- エントロピー ↔ 仮説の多様性

のバランスを取っている。

### 「起こりやすい状態」を選ぶという共通点

ここで重要な共通理解がある。

統計力学では：

- 最も低エネルギーな状態（誤）
- **最も起こりやすい状態分布**（正）

が選ばれる。

ベイズ変分推論では：

- 最も尤度が高い仮説（誤）
- **最もあり得る仮説分布**（正）

が選ばれる。

どちらも、**「極端な最小値」ではなく「現実に発生しやすい分布」** を選んでいる。

### 温度パラメータの対応関係

統計力学では温度 $${{T}}$$ が、

- エネルギーをどれくらい厳しく見るか
- エントロピーをどれくらい許容するか

を決める。

- $${{T \to 0}}$$：最低エネルギー状態に集中
- $${{T \to \infty}}$$：エネルギーを無視して一様分布へ

ベイズ変分推論では、標準的には温度は暗黙に $${{T=1}}$$ に相当する形で扱われている。

しかし対応を明示すると：

$$
\mathcal{F}_T[q] = \mathbb{E}_q[-\log p(x,\theta)] + T,\mathbb{E}_q[\log q(\theta)]
$$

と書ける。これは：

- Tempered Bayes
- β-VAE
- Annealed Variational Inference

などとして実際に使われている。

意味としては：

- 低温（小さい $${{T}}$$）：仮説を強く絞る（確信的）
- 高温（大きい $${{T}}$$）：不確実性を残す（探索的）

**推論の「硬さ・柔らかさ」を決めるパラメータ**である。

### なぜここまで一致するのか

これは偶然ではない。

- **統計力学**：見えないミクロ状態を平均した結果として、マクロが決まる
- **ベイズ推論**：見えない真の仮説を平均した結果として、信念が決まる

どちらも、

> 「見えないものを直接選べない」→「分布として扱うしかない」

という制約から生まれている。

統計力学の自由エネルギーも、ベイズ変分自由エネルギーも、**「最も起こりやすい世界の分布」を選ぶための原理**であり、温度とは、その選び方の厳しさを調整するノブである。

## 7. Fristonの一歩先

Fristonが主張したのは：

> **生命は、驚きを直接下げられないため、その上界（自由エネルギー）を知覚・学習・行動によって最小化する存在である**

つまり変分原理は：

- 物理の原理
- 推論の原理
- 認知・行動の原理

として**一本につながる**。


## 8. 一文で総括

> **変分自由エネルギーとは、「世界は直接分からない」という制約の下で、驚きを最小化するために必然的に現れる量である。**


## 注意点・限界

本記事で示した数学的導出は厳密ですが、以下の点に注意が必要です：

- ここでの「自由エネルギー」は統計的な量であり、物理量そのものではない
- モデル誤指定時、自由エネルギーの最小化が現実への適応を保証するとは限らない
- Fristonの生物学的主張は魅力的な仮説だが、実験的検証が進行中の段階
- 数学的同型性と物理的同一性は概念的に区別が必要


## 参考文献

- Kolmogorov, A. N. (1933). [_Foundations of the Theory of Probability_](https://archive.org/details/foundationsofthe00kolm) (English translation 1950, Chelsea Publishing)
- Shannon, C. E. (1948). [A Mathematical Theory of Communication](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf). _Bell System Technical Journal_, 27(3), 379-423
- Bishop, C. M. (2006). [_Pattern Recognition and Machine Learning_](https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/), Chapter 10 (Springer)
- MacKay, D. J. C. (2003). [_Information Theory, Inference, and Learning Algorithms_](http://www.inference.org.uk/mackay/itila/) (Cambridge University Press) - 著者による無料公開版
- Jaynes, E. T. (2003). [_Probability Theory: The Logic of Science_](https://bayes.wustl.edu/etj/prob/book.pdf) (Cambridge University Press)
- Gibbs, J. W. (1902). [_Elementary Principles in Statistical Mechanics_](https://archive.org/details/elementaryprinc00gibbgoog) (Internet Archive)
- Friston, K. (2010). [The free-energy principle: a unified brain theory?](https://doi.org/10.1038/nrn2787) _Nature Reviews Neuroscience_, 11(2), 127-138


## 付録：KL分解からの詳細導出

前半ではJensen不等式から自由エネルギーを導出したが、KL分解から出発する方法もある。以下では、この別アプローチを詳細に示す。

### A1. 出発点：任意の q(θ) と事後のKL

$$
\mathrm{KL}\bigl(q(\theta)|p(\theta\mid x)\bigr) = \int q(\theta)\log\frac{q(\theta)}{p(\theta\mid x)}d\theta
$$

KLの定義そのものである。


### A2. 事後分布をベイズで展開

ベイズの定理：

$$
p(\theta\mid x)=\frac{p(x,\theta)}{p(x)}
$$

これをKLに代入する：

$$
\mathrm{KL}(q|p(\theta\mid x)) = \int q(\theta)\log\frac{q(\theta)}{p(x,\theta)/p(x)}d\theta
$$

分母の割り算を整理して：

$$
= \int q(\theta)\log\frac{q(\theta)p(x)}{p(x,\theta)}d\theta
$$


### A3. 対数を分配して、定数を外に出す

$$
\log\frac{q(\theta)p(x)}{p(x,\theta)} = \log q(\theta) + \log p(x) - \log p(x,\theta)
$$

よって：

$$
\mathrm{KL}(q|p(\theta\mid x)) = \int q(\theta)\bigl(\log q(\theta) + \log p(x) - \log p(x,\theta)\bigr)d\theta
$$

ここで $${{\log p(x)}}$$ は $${{\theta}}$$ に依らない定数なので：

$$
= \int q(\theta)\log q(\theta)d\theta + \log p(x)\int q(\theta)d\theta - \int q(\theta)\log p(x,\theta)d\theta
$$

確率分布なので $${{\int q(\theta)d\theta = 1}}$$。したがって：

$$
= \underbrace{\int q(\theta)\log q(\theta)d\theta}_{\mathbb{E}_q[\log q(\theta)]} + \log p(x)
$$

$$
-\underbrace{\int q(\theta)\log p(x,\theta),d\theta}_{\mathbb{E}_q[\log p(x,\theta)]}
$$


### A4. 変分自由エネルギーを定義して整理

ここで、慣習どおり

$$
\mathcal{F}(q) \equiv \mathbb{E}_q[-\log p(x,\theta)] + \mathbb{E}_q[\log q(\theta)]
$$

と置くと、上の式は

$$
\mathrm{KL}(q|p(\theta\mid x)) = \mathcal{F}(q) + \log p(x)
$$

（※ $${{\mathbb{E}_q[-\log p] = -\mathbb{E}_q[\log p]}}$$ を使っている）

よって

$$
\mathcal{F}(q) = -\log p(x) + \mathrm{KL}\bigl(q(\theta)|p(\theta\mid x)\bigr)
$$

これが欲しかった恒等式である。


### A5. 「上界」はどこから来る？（Jensenなしでも言える）

KLの基本性質：

$$
\mathrm{KL}(\cdot|\cdot)\ge 0
$$

したがって直ちに：

$$
\mathcal{F}(q)\ge -\log p(x)
$$

つまり、自由エネルギーは驚きの上界。


### A6. 何が違うのか（Jensen経由との比較）

- **KLからの導出**：「自由エネルギー＝驚き＋近似誤差」という分解が最初から見える
- **Jensenからの導出**：「驚きの上界として自然に出る」が一撃で分かる

どちらも同じ結論に到達する。


### 付録の注意点

- この導出は「事後 $${{p(\theta\mid x)}}$$」を式として書けることが前提（ただし計算可能とは限らない）
- 実際の変分推論では $${{\mathcal{F}(q)}}$$ を評価できる形に落とす必要がある（平均場など）
- 応用上のモデル選択や正則化は専門家に確認が必要


## おわりに

本記事では、ベイズの定理から出発して変分自由エネルギーが必然的に導出されるプロセスを追いかけました。この数学的構造は統計力学の自由エネルギーと完全に同型であり、Fristonの自由エネルギー原理は、この同型性を認知・行動・生命の原理として大胆に再解釈したものです。

この導出を追う中で、個人的にいちばん心を打たれたのは次の点でした。

統計物理学では、系は「最もエネルギーが低い状態」ではなく、エネルギーとエントロピーのトレードオフの結果として最も"起こりやすい"状態分布をとります。

そしてFristonの解釈では、生物もまた、世界をどう理解するか（知覚・学習）とどこへ動くか（行動）を通じて、自分にとって起こりやすい世界を維持するように振る舞う存在として描かれます。

つまり、分子が熱浴の中で自由エネルギーを下げるのと、生物が環境の中で驚きを下げようとすることが、数学的に同じ原理で書けてしまう。

この一致は比喩ではなく、統計力学の変分原理と統計推論の数学的同型性、そしてFristonによる認知・行動への大胆な拡張から来ています。

「生命とは何か」「知るとは何か」「動くとは何か」という問いが、ベイズの定理と自由エネルギーという、驚くほどシンプルな数式の上に重なって見えてくる。

その瞬間、世界はずいぶん美しくできているなと感じずにはいられませんでした。

改めて、本記事はChatGPT 5.2との対話を通じた学習の記録です。誤りや改善点があればぜひご指摘ください。議論や質問も歓迎します。

（初版：2026-01-15）

### 更新履歴
<!-- CHANGELOG_MARKER -->
- 2026-01-19: [生成仮定→生成過程のtypo修正](https://github.com/ikuo-suyama/bayes-to-friston-free-energy/pull/4)